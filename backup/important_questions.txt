



    #!/usr/bin/env python3
import re, io, json
from pathlib import Path
import fitz                     # PyMuPDF
from PIL import Image
import pytesseract
import ollama

# ============== CONFIG ==============
PDF_PATH = Path(r"past papers/SET1.pdf")   # change per set
OUT_DIR  = Path("./output/SET1")           # change per set
LLM_MODEL = "llama3"                       # ollama chat model
OCR_MIN_CHARS = 120
# ====================================

UNITS = [
 "Unit 1.1 Networking and Telecommunication",
 "Unit 1.2 Ethical and Social Issues in ICT",
 "Unit 1.3 Computer Security",
 "Unit 1.4 E-commerce",
 "Unit 1.5 Contemporary Technology",
 "Unit 1.6 Number System",
 "Unit 2.1 Database Management System",
 "Unit 3.1 Programming in QBASIC",
 "Unit 3.2 Modular Programming",
 "Unit 3.3 File Handling in QBASIC",
 "Unit 4.1 Structured Programing in C",
]

# ---------------- OCR extraction ----------------
def extract_text_with_ocr(pdf_path: Path):
    doc = fitz.open(str(pdf_path))
    page_texts = []
    for i in range(doc.page_count):
        page = doc.load_page(i)
        txt = page.get_text("text") or ""
        if len(txt.strip()) < OCR_MIN_CHARS:
            pix = page.get_pixmap(dpi=300)
            img = Image.open(io.BytesIO(pix.tobytes("png")))
            txt += "\n" + pytesseract.image_to_string(img, config="--oem 3 --psm 6")
        page_texts.append(txt.replace("\u00A0"," ").strip())
    doc.close()
    return "\n".join(page_texts)

# ---------------- LLM helpers ----------------
def llm_extract_questions(raw_text: str):
    sys_msg = "You extract atomic exam questions. Return only JSONL with fields {id, text}."
    user_msg = (
        "Extract all exam questions from the following text. "
        "Split multipart (a)(b)(c) into atomic questions.\n\n" + raw_text[:12000]
    )
    res = ollama.chat(model=LLM_MODEL, messages=[
        {"role":"system","content":sys_msg},
        {"role":"user","content":user_msg}
    ], options={"temperature":0})
    qs=[]
    for line in res["message"]["content"].splitlines():
        try: qs.append(json.loads(line))
        except: pass
    return qs

def llm_classify_chapter(qtext: str) -> str:
    prompt = f"Assign this exam question to one of the following units:\n{UNITS}\n\nQuestion: {qtext}\nReturn only the exact unit string."
    res = ollama.chat(model=LLM_MODEL, messages=[{"role":"user","content":prompt}], options={"temperature":0})
    ans = res["message"]["content"].strip()
    for u in UNITS:
        if u.lower() in ans.lower(): return u
    return "Unknown"

def llm_is_important(qtext: str) -> bool:
    prompt = f"Based on exam trends, is this an IMPORTANT question? Answer Yes or No.\n\n{qtext}"
    res = ollama.chat(model=LLM_MODEL, messages=[{"role":"user","content":prompt}], options={"temperature":0})
    return "yes" in res["message"]["content"].lower()

# ---------------- Main ----------------
def main():
    if not PDF_PATH.exists():
        raise SystemExit(f"PDF not found: {PDF_PATH}")
    OUT_DIR.mkdir(parents=True, exist_ok=True)

    print(f"[parse] {PDF_PATH.name}")
    text = extract_text_with_ocr(PDF_PATH)
    raw_qs = llm_extract_questions(text)

    final=[]
    for i,q in enumerate(raw_qs,1):
        t=q.get("text","").strip()
        if not t: continue
        chap = llm_classify_chapter(t)
        imp  = llm_is_important(t)
        if not imp: continue
        final.append({
            "id": f"{PDF_PATH.stem}::Q{i:04d}",
            "paper_id": PDF_PATH.stem,
            "text": t,
            "chapter": chap,
        })

    # write outputs
    with (OUT_DIR/"questions.jsonl").open("w",encoding="utf-8") as f:
        for q in final:
            f.write(json.dumps(q,ensure_ascii=False)+"\n")

    with (OUT_DIR/"important_all_chapters.json").open("w",encoding="utf-8") as f:
        by_chap={}
        for q in final:
            by_chap.setdefault(q["chapter"],[]).append(q)
        json.dump(by_chap,f,indent=2,ensure_ascii=False)

    print(f"[done] Wrote {len(final)} important questions to {OUT_DIR}")

if __name__=="__main__":
    main()
